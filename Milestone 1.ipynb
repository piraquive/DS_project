{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Names\n",
    "Diego Piraquive \n",
    "James Firpo\n",
    "Trevor Teerlink\n",
    "\n",
    "### Project:  Professor Research vs Student Success\n",
    "\n",
    "To refresh, the purpose of our project is to search for any correlations between the amount of time a professor spends on research, and detect if this has an effect on their students.  We have a suspicion that professors who are heavily involved in research, have less commitment to their students, which may be reflected through poorer student performance.\n",
    "\n",
    "We have two source we are scraping from.  The OBIA (Office of Budget and Institutional Analysis), and also a factory directory website, which contains links to every professor's Google Scholar account.  \n",
    "\n",
    "From the OBIA we will scrape classes, the class professor, and the class grades.\n",
    "From the Faculty directory and google scholar links, we will attain an \"h-index\" for each professor.\n",
    "The h-index is an author-level metric that attempts to measure both the productivity and citation impact of the publications of a scientist or scholar. The index is based on the set of the scientist's most cited papers and the number of citations that they have received in other publications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we demonstrate we can scrape a common faculty directory, https://mech.utah.edu/faculty/.\n",
    "# The code builds two functions:  one function to get the h-index from google scholar, and the second function\n",
    "# navigates to grab the google scholar url from the faculty page.\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def get_hindex(url):\n",
    "    scholar_soup = BeautifulSoup(urllib.request.urlopen(url), 'lxml')\n",
    "    return int(scholar_soup.findAll(\"td\", {\"class\": \"gsc_rsb_std\"})[2].text)\n",
    "\n",
    "\n",
    "\n",
    "def get_prof_scores():\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(\"https://mech.utah.edu/faculty/\"), 'lxml')\n",
    "\n",
    "    for row in soup.find(\"table\", {\"id\": \"tablepress-7\"}).findAll(\"tr\"):\n",
    "        tds = row.findAll(\"td\")\n",
    "        if len(tds) == 0:\n",
    "            continue\n",
    "\n",
    "        _, name, body, _ = tds\n",
    "        link = body.find(\"a\")\n",
    "        if not link:\n",
    "            continue\n",
    "        url = link[\"href\"]\n",
    "      \n",
    "\n",
    "        yield (name.find(\"strong\").text.strip(), get_hindex(url))\n",
    "\n",
    "prof_scores = list(get_prof_scores())\n",
    "\n",
    "print(prof_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we have a list of all the Mechanical Engineering professors and their h-index score.  This is a very handy\n",
    "# metric to show how \"dedicated\" a professor is to research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we have code to index all the engineering classes from the OBIA site. \n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) \n",
    "from bs4 import UnicodeDammit\n",
    "In [2]:\n",
    "def getgrades(csv1, csv2):\n",
    "    grades1=pd.read_csv(csv1, encoding=\"utf8\")\n",
    "    grades2=pd.read_csv(csv2, encoding=\"utf8\")\n",
    "    grds=[grades1, grades2]\n",
    "    grades=pd.concat(grds,ignore_index=True)\n",
    "    i=0\n",
    "    grades=grades.reset_index(drop=True)\n",
    "    hc=grades['sumHeadcount']\n",
    "    while i < len(hc):\n",
    "        if hc[i]== 'ds':\n",
    "            grades=grades.drop(i, axis=0)\n",
    "        i+=1\n",
    "    grades=grades.reset_index(drop=True)\n",
    "    return(grades)\n",
    "In [3]:\n",
    "def combinegrades(grades):\n",
    "    i=1\n",
    "    n=2\n",
    "    gradesclean=pd.DataFrame([],columns=['Num', 'Section', 'Subject', 'A', 'B', 'C', 'D', 'E', 'W', 'Other'])\n",
    "    gradesclean=gradesclean.append({'Num':1050, 'Section':1, 'Subject':'ASTR - Astronomy', 'A':0, 'B':0, 'C':0, 'D':0,\n",
    "                                    'E':0, 'W':0, 'Other':2}, ignore_index=True)\n",
    "    while i<len(grades['sumHeadcount']):\n",
    "        if grades['Subject'].iloc[i]==gradesclean['Subject'].iloc[-1]:\n",
    "            if grades['Catalog Num'].iloc[i]==gradesclean['Num'].iloc[-1]:\n",
    "                fail=0\n",
    "                for m in range(1,n):\n",
    "                    if grades['Section'].iloc[i]==gradesclean['Section'].iloc[-m]:\n",
    "                        gradesclean[grades['Grade Group'].iloc[i]].iat[-m]=grades['sumHeadcount'].iloc[i]\n",
    "                        break\n",
    "                    fail+=1\n",
    "                if fail==n-1:\n",
    "                    n+=1\n",
    "                    gradesclean=gradesclean.append({'Num':grades['Catalog Num'].iloc[i],\n",
    "                                                    'Section':grades['Section'].iloc[i],\n",
    "                                                    'Subject':grades['Subject'].iloc[i], 'A':0, 'B':0,\n",
    "                                                    'C':0,'D':0, 'E':0, 'W':0, 'Other':0}, ignore_index=True)\n",
    "                    gradesclean[grades['Grade Group'].iloc[i]].iat[-1]=grades['sumHeadcount'].iloc[i]\n",
    "            else:\n",
    "                n=2\n",
    "                gradesclean=gradesclean.append({'Num':grades['Catalog Num'].iloc[i], 'Section':grades['Section'].iloc[i],\n",
    "                                                'Subject':grades['Subject'].iloc[i], 'A':0, 'B':0,\n",
    "                                                'C':0,'D':0, 'E':0, 'W':0, 'Other':0}, ignore_index=True)\n",
    "                gradesclean[grades['Grade Group'].iloc[i]].iat[-1]=grades['sumHeadcount'].iloc[i]\n",
    "        else:\n",
    "            n=2\n",
    "            gradesclean=gradesclean.append({'Num':grades['Catalog Num'].iloc[i], 'Section':grades['Section'].iloc[i],\n",
    "                                            'Subject':grades['Subject'].iloc[i], 'A':0, 'B':0,\n",
    "                                            'C':0,'D':0, 'E':0, 'W':0, 'Other':0}, ignore_index=True)\n",
    "            gradesclean[grades['Grade Group'].iloc[i]].iat[-1]=grades['sumHeadcount'].iloc[i]\n",
    "        i+=1\n",
    "    return(gradesclean)\n",
    "In [5]:\n",
    "sgrades=['fall17sci.csv','spring17sci.csv']\n",
    "egrades=['fall17eng.csv','spring17eng.csv']\n",
    "i=0\n",
    "cleaned=[0,0]\n",
    "while i<len(sgrades):\n",
    "    gradess=getgrades(sgrades[i],egrades[i])\n",
    "    gradescleans=combinegrades(gradess)\n",
    "    cleaned[i]=gradescleans\n",
    "    i+=1\n",
    "In [7]:\n",
    "cleaned[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Relating professor names to class numbers\n",
    "We used the catalog as we did in lecture in order to relate the class numbers to the professors. We manually modified the html code for MATH and ME EN subjects. We are going to explore using Selenium for when we scrape all subjects in order to have the code work more for us and automate it better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_soup = BeautifulSoup(open(\"S17_MATH_class_list.html\"), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = pd.read_html(str(class_table))[0]\n",
    "classes.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleanup data and get rid of columns \n",
    "mask = pd.notnull(classes[\"Sec.\"])\n",
    "classes[mask]\n",
    "\n",
    "classes = classes[(classes[\"Component\"] == \"Lecture\") | (classes[\"Component\"] == \"Seminar\") | (classes[\"Component\"] == \"Special Topics\")]\n",
    "\n",
    "classes = classes.drop(['Units','Location',\"Class Attrs\",\"Feed back\",'Pre Req','Fees'],axis=1)\n",
    "In [148]:\n",
    "classes = classes.drop(['Days/Time & Session'],axis=1)\n",
    "In [149]:\n",
    "classes = classes.reset_index()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code exports the dataframe into an excel file. this way we can integrate it into other sections of our code better\n",
    "writer = pd.ExcelWriter('S17_math_classes.xlsx')\n",
    "In [141]:\n",
    "S17_math_classes = classes\n",
    "S17_math_classes.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONCLUSION\n",
    "\n",
    "We now can begin to analyze our data, and search correlations within.  We plan on using correlations factors to determine if there is postive or negative correlations between the h-index variable of a professor, and their average class grade.  One Problem we are fixing is the innability to scrape sites past what you see without scrolling.  Just today we found out we need to use a function called Silenium to grab all the html from a page, which grabs the html to the bottom of the scroll.  As a back up, we also have considered looking at student feedback scores that are given to professors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
